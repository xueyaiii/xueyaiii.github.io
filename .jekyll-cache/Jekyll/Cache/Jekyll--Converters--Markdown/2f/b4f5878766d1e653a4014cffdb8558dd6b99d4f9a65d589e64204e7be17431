I"2<ul id="markdown-toc">
  <li><a href="#前言" id="markdown-toc-前言">前言</a></li>
  <li><a href="#设计目标" id="markdown-toc-设计目标">设计目标</a></li>
  <li><a href="#框架使用说明" id="markdown-toc-框架使用说明">框架使用说明</a></li>
  <li><a href="#框架环境" id="markdown-toc-框架环境">框架环境</a></li>
  <li><a href="#框架完善" id="markdown-toc-框架完善">框架完善</a></li>
</ul>

<h2 id="前言">前言</h2>

<p>该文档针对爬虫系统设计目标中相应的场景给出技术方案</p>

<h2 id="设计目标">设计目标</h2>

<p>1、代码复用，功能模块化。可以支持上千个网站的数据爬取；</p>

<p>2、易扩展。爬虫框架易扩展，爬取规则、解析规则、入库规则易扩展，支持框架切换；</p>

<p>3、健壮性、可维护性。对数据爬取过程中的各种异常，例如：断网、反爬升级、爬“脏数据”等，需要实时的监控，以及给出准确的定位。异常处理以及降级措施需要完善；</p>

<p>4、后续扩展为分布式结构；</p>

<p>5、支持功能模块的易调整；</p>

<h2 id="框架使用说明">框架使用说明</h2>

<p><strong>News_scrapy_redis4. <a href="https://github.com/xudailong/News_scrapy_redis.git">github地址</a></strong></p>

<ol>
  <li>
    <p><code class="highlighter-rouge">News_scrapy_redis</code> 基于<code class="highlighter-rouge">scrapy_redis</code>实现数据的增量爬取（含去重），支持分布式，支持异常日志等输出，功能模块化。</p>
  </li>
  <li>
    <p>树结构：</p>
  </li>
</ol>

<div class="language-js highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="err">├─</span><span class="p">.</span><span class="nx">idea</span>
<span class="err">├─</span><span class="nx">Daily_crawler</span>
<span class="err">├─</span><span class="nx">ETL</span>
<span class="err">├─</span><span class="nx">log</span>
<span class="err">├─</span><span class="nx">News_scrapy</span>
<span class="err">│</span>  <span class="err">├─</span><span class="nx">spiders</span>
<span class="err">│</span>  <span class="err">│</span>  <span class="err">└─</span><span class="nx">__pycache__</span>
<span class="err">│</span>  <span class="err">└─</span><span class="nx">__pycache__</span>
<span class="err">├─</span><span class="nx">News_simhash</span>
<span class="err">└─</span><span class="nx">News_statistics</span>

</code></pre></div></div>

<ol>
  <li>
    <p>各模块说明：</p>

    <blockquote>
      <p>Daily_crawler：</p>
    </blockquote>

    <ul>
      <li><code class="highlighter-rouge">daily_crawler.cron crontab</code>的定时文件, 定时运行<code class="highlighter-rouge">start_crawl.sh</code>脚本</li>
      <li><code class="highlighter-rouge">start_crawl.sh</code> 启动爬虫模块，并将每次爬取所花费的时间 写入 log/run_time.txt</li>
      <li><code class="highlighter-rouge">push_urls.py</code> 每次在爬虫之前运行，清空调度队列，并将start_url push到调度队列中</li>
      <li><code class="highlighter-rouge">news_crawl.sh</code> 执行爬虫模块（增量爬取）， 并自动进行相似文档去重，ETL, 存入mongodb</li>
    </ul>

    <blockquote>
      <p>ETL:（暂时用不到）</p>
    </blockquote>

    <ul>
      <li><code class="highlighter-rouge">/Model</code> 存放训练好的词典，语料，TF-IDF，LDA， word2vec模型</li>
      <li><code class="highlighter-rouge">auto_embedding.py</code> 新闻语料的清洗，以及自动化生成新闻的标题和内容embedding</li>
      <li><code class="highlighter-rouge">auto_embedding_simhash.py</code> 增加了自动化相似文档的去重</li>
      <li><code class="highlighter-rouge">stop_words</code> 常用的中文停留词</li>
      <li><code class="highlighter-rouge">train_step1</code> 训练LDA模型</li>
      <li><code class="highlighter-rouge">train_step2</code> 训练LDA模型</li>
    </ul>

    <blockquote>
      <p>log:</p>
    </blockquote>

    <ul>
      <li><code class="highlighter-rouge">auto_embedding_simhash.log</code> 执行auto_embedding_simhash.py的日志文件</li>
      <li><code class="highlighter-rouge">crawler.log</code> 执行scrapy-redis爬虫模块的日志文件</li>
      <li><code class="highlighter-rouge">news_count.log</code> 执行news_statistics.py的日志文件</li>
      <li><code class="highlighter-rouge">run_time.txt</code> 每次执行爬虫脚本的运行时间</li>
    </ul>

    <blockquote>
      <p>News_data:</p>
    </blockquote>

    <ul>
      <li>每个文件夹是抓每天从各个网站抓取到的新闻</li>
    </ul>

    <blockquote>
      <p>News_scrapy:</p>
    </blockquote>

    <ul>
      <li>基于scrapy-redis的爬虫模块，在scrapy的基础上修改得到</li>
      <li>各大网站数据的爬取解析工作主要在该文件中进行</li>
    </ul>

    <blockquote>
      <p>News_simhash（此处只需要进行title的去重）:</p>
    </blockquote>

    <ul>
      <li>实现相似文档的去重</li>
      <li>automatic_simhash.py 自动实现相似文档的去重（仅基于新闻内容）</li>
      <li>content_index.pkl 序列化的新闻内容SimhashIndex类，相当于所有新闻sinhash_value的一张表，并且随着抓取的新闻越来越多，该表会不断增大title_index.pkl 序列化的新闻标题SimhashIndex类， 未使用</li>
      <li><code class="highlighter-rouge">generate_simhash_index.py</code> 初始化这张Simhash_index，生成content和title的Simhash_index</li>
      <li><code class="highlighter-rouge">near_duplicates.py</code> 对初始化的Simhash_index进行相似新闻内容的去重</li>
      <li>test.py 测试content_index.pkl中content_index类新闻的数量</li>
    </ul>

    <blockquote>
      <p>News_statistics:</p>
    </blockquote>

    <ul>
      <li><code class="highlighter-rouge">news_count.json</code> 每天从各个网站抓取的新闻数量</li>
      <li><code class="highlighter-rouge">news_statistics.py</code> 统计新闻增量的脚本</li>
    </ul>
  </li>
</ol>

<h2 id="框架环境">框架环境</h2>

<ol>
  <li>Redis环境环境</li>
  <li>scrapy框架环境</li>
  <li>python3环境环境</li>
</ol>

<h2 id="框架完善">框架完善</h2>
<ol>
  <li>IP代理池</li>
  <li>cookies池</li>
  <li>其他</li>
</ol>

:ET